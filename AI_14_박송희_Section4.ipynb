{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CovnIsiNSDcL"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade efficientnet-pytorch"
      ],
      "metadata": {
        "id": "fcMew_Tqb0wT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from PIL import Image\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import numpy as np\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from efficientnet_pytorch import EfficientNet"
      ],
      "metadata": {
        "id": "gISUftk3coPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "FqJVCnBMcA1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_train_val(data_dir, val_size=0.1, random_state=42):\n",
        "    train, test = [], []\n",
        "    for dset in os.listdir(data_dir):\n",
        "        # check is directory\n",
        "        subdir = os.path.join(data_dir, dset)\n",
        "        if os.path.isdir(subdir):\n",
        "            for label in os.listdir(subdir):\n",
        "                imgdir = os.path.join(subdir, label)\n",
        "                if os.path.isdir(imgdir):\n",
        "                    for image_path in os.listdir(imgdir):\n",
        "                        if image_path.endswith(\".jpg\"):\n",
        "                            sample = {\n",
        "                                \"path\": os.path.join(subdir, label, image_path),\n",
        "                                \"label\": label\n",
        "                            }\n",
        "                            if dset == \"training_set\":\n",
        "                                train.append(sample)\n",
        "                            elif dset == \"testing_set\":\n",
        "                                test.append(sample)\n",
        "    \n",
        "    train = pd.DataFrame(train)\n",
        "    test = pd.DataFrame(test)\n",
        "    train, val = train_test_split(train, test_size=val_size, random_state=random_state)\n",
        "    \n",
        "    return train, val, test"
      ],
      "metadata": {
        "id": "DYzfRoM3pyH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, val_df, test_df = split_train_val(\"/content/drive/MyDrive/FaceShape Dataset/\")\n",
        "test_df"
      ],
      "metadata": {
        "id": "neM9UXxXV4fx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt  \n",
        "\n",
        "def display_examples():\n",
        "    \n",
        "    \"\"\"\n",
        "        Display 25 images from the images and labels\n",
        "    \"\"\"\n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "    fig.suptitle(\"Some examples of images of the dataset\", fontsize=16)\n",
        "    for i in range(25):\n",
        "        plt.subplot(5,5,i+1)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        plt.grid(False)\n",
        "        img_path = train_df.iloc[i][\"path\"]\n",
        "        image = cv2.imread(img_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        \n",
        "        plt.imshow(image, cmap=plt.cm.binary)\n",
        "        plt.xlabel(train_df.iloc[i][\"label\"])\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "0K1kFk-Rqb8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_examples()"
      ],
      "metadata": {
        "id": "6buHiqrfohl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset\n",
        "class FaceShapeDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, df, transform=None, split=\"train\"):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.transform = transform\n",
        "        self.split = split\n",
        "        class_names = ['Heart', 'Oblong', 'Oval', 'Round', 'Square']\n",
        "        self.label2idx = {class_name:i for i, class_name in enumerate(class_names)}\n",
        "        self.idx2label = {v:k for k,v in self.label2idx.items()}\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "   \n",
        "    def __getitem__(self, idx):\n",
        "        try:\n",
        "            img_path = self.df.loc[idx, \"path\"]\n",
        "            img = Image.open(img_path).convert('L')\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "            \n",
        "            label = self.df.loc[idx, \"label\"]\n",
        "            label = self.label2idx[label]\n",
        "            return img, torch.tensor(label)\n",
        "        except:\n",
        "            print(f\"Error load image {img_path}\")\n",
        "            idx = 0\n",
        "            img_path = self.df.loc[idx, \"path\"]\n",
        "            img = Image.open(img_path).convert('L')\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "            \n",
        "            label = self.df.loc[idx, \"label\"]\n",
        "            label = self.label2idx[label]\n",
        "            return img, torch.tensor(label)"
      ],
      "metadata": {
        "id": "OEjJaK39R3Za"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Models\n",
        "class EffNet(nn.Module):\n",
        "    def __init__(self, num_classes=5):\n",
        "        super(EffNet, self).__init__()\n",
        "        self.eff = EfficientNet.from_pretrained('efficientnet-b5', num_classes=num_classes, in_channels=1)\n",
        "    def forward(self, x):\n",
        "        x = self.eff(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "4UdPxWEjbWC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, val_loader, criterion, optimizer, epochs, device):\n",
        "    best_accuracy = 0\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=\"Training Epoch {}\".format(epoch+1))\n",
        "        for batch_idx, (data, target) in bar:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            bar.set_postfix(loss=loss.item())\n",
        "        \n",
        "        \n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        correct = 0\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            bar = tqdm(enumerate(val_loader), total=len(val_loader), desc=\"Validation Epoch {}\".format(epoch+1))\n",
        "            for batch_idx, (data, target) in bar:\n",
        "                data, target = data.to(device), target.to(device)\n",
        "                output = model(data)\n",
        "                val_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
        "                pred = output.argmax(dim=1, keepdim=True)\n",
        "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "        val_loss /= len(val_loader.dataset)\n",
        "        val_accuracy = 100. * correct / len(val_loader.dataset)\n",
        "        print('\\nValidation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "            val_loss, correct, len(val_loader.dataset),\n",
        "            val_accuracy))\n",
        "        \n",
        "        # save best model\n",
        "        if val_accuracy > best_accuracy:\n",
        "            best_accuracy = val_accuracy\n",
        "            torch.save(model.state_dict(), \"./best_model.pth\")\n",
        "            print(\"Saved best model\")\n",
        "        print(\"Best accuracy: {}\".format(best_accuracy))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "7zY9R9ySbWzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configs\n",
        "class args:\n",
        "    data_dir=\"/content/drive/MyDrive/FaceShape Dataset/\"\n",
        "    batch_size=32\n",
        "    n_epochs=20\n",
        "    learning_rate=0.001\n",
        "    debug=False"
      ],
      "metadata": {
        "id": "kUVvmzw9bZJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, val_df, test_df = split_train_val(args.data_dir)\n",
        "if args.debug:\n",
        "    train_df, val_df = train_df.sample(n=10).reset_index(drop=True), val_df.sample(n=10).reset_index(drop=True)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    FaceShapeDataset(train_df, transform=transform, split=\"train\"),\n",
        "    batch_size=args.batch_size, shuffle=True\n",
        ")\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    FaceShapeDataset(val_df, transform=transform, split=\"val\"),\n",
        "    batch_size=args.batch_size, shuffle=False\n",
        ")\n",
        "\n",
        "import time\n",
        "from datetime import timedelta\n",
        "st = time.time()\n",
        "print(\"-------- Start training --------\")\n",
        "model = EffNet().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
        "model = train(model, train_loader, val_loader, criterion, optimizer, epochs=args.n_epochs, device=device)\n",
        "print(\"-------- End training, time taken:\", timedelta(seconds=int(time.time()-st)))"
      ],
      "metadata": {
        "id": "05GE9Jo5bayH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_ds = FaceShapeDataset(test_df, transform=transform, split=\"test\")\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_ds,\n",
        "    batch_size=args.batch_size, shuffle=False\n",
        ")\n",
        "\n",
        "model.eval()\n",
        "test_loss = 0\n",
        "correct = 0\n",
        "y_true = []\n",
        "y_pred = []\n",
        "with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        output = model(data)\n",
        "        test_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
        "        pred = output.argmax(dim=1, keepdim=True)\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "        y_true += target.view(-1).tolist()\n",
        "        y_pred += pred.view(-1).tolist()\n",
        "test_loss /= len(test_loader.dataset)\n",
        "print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "    test_loss, correct, len(test_loader.dataset),\n",
        "    100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "y_true = [test_ds.idx2label[i] for i in y_true]\n",
        "y_pred = [test_ds.idx2label[i] for i in y_pred]\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_true, y_pred))"
      ],
      "metadata": {
        "id": "2MG9UKWMcOmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sn; sn.set(font_scale=1.4)\n",
        "\n",
        "CM = confusion_matrix(y_true, y_pred)\n",
        "class_names = list(test_ds.label2idx.keys())\n",
        "ax = plt.axes()\n",
        "sn.heatmap(CM, annot=True, \n",
        "           annot_kws={\"size\": 10}, \n",
        "           xticklabels=class_names, \n",
        "           yticklabels=class_names, ax = ax)\n",
        "ax.set_title('Confusion matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fZenVFDP0Ozj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from random import randint\n",
        "\n",
        "list_idx = [randint(0, len(test_ds)) for i in range(25)]\n",
        "\n",
        "def display_examples():\n",
        "    \n",
        "    \"\"\"\n",
        "        Display 25 images from the images and labels\n",
        "    \"\"\"\n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "    fig.suptitle(\"Some examples of images of the test set\", fontsize=16)\n",
        "    for i,idx in enumerate(list_idx):\n",
        "        plt.subplot(5,5,i+1)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        plt.grid(False)\n",
        "        img_path = test_ds.df.iloc[idx][\"path\"]\n",
        "        image = cv2.imread(img_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        \n",
        "        plt.imshow(image, cmap=plt.cm.binary)\n",
        "        plt.xlabel(y_true[idx])\n",
        "    plt.show()\n",
        "\n",
        "display_examples()"
      ],
      "metadata": {
        "id": "Ah_RxhYJ0PIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_examples():\n",
        "    \n",
        "    \"\"\"\n",
        "        Display 25 images from the images and labels\n",
        "    \"\"\"\n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "    fig.suptitle(\"Some examples of predictive images of the test set\", fontsize=16)\n",
        "    for i,idx in enumerate(list_idx):\n",
        "        plt.subplot(5,5,i+1)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        plt.grid(False)\n",
        "        img_path = test_ds.df.iloc[idx][\"path\"]\n",
        "        image = cv2.imread(img_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        \n",
        "        plt.imshow(image, cmap=plt.cm.binary)\n",
        "        plt.xlabel(y_pred[idx])\n",
        "    plt.show()\n",
        "\n",
        "display_examples()"
      ],
      "metadata": {
        "id": "xg9LquEG0jUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ⬇얼굴 크롭"
      ],
      "metadata": {
        "id": "4EWpUC-U1aNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from glob import glob\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "faceCascade = cv2.CascadeClassifier('/content/drive/MyDrive/haarcascade_frontalface_default.xml')\n",
        "\n",
        "face_name = ['Heart','Oblong','Oval','Round','Square']\n",
        "\n",
        "for face in face_name:\n",
        "    image_datas = glob('/content/drive/MyDrive/FaceShape Dataset/training_set/'+face+'/*.jpg')\n",
        "    imgNum  = 0\n",
        "    for imgpath in image_datas:\n",
        "        print(imgpath)\n",
        "        image = cv2.imread(imgpath)\n",
        "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "        faces = faceCascade.detectMultiScale(\n",
        "            gray,\n",
        "            scaleFactor=1.3,\n",
        "            minNeighbors=5,\n",
        "            minSize=(30, 30)\n",
        "        )\n",
        "        tem_w = 0\n",
        "        \n",
        "        for (x, y, w, h) in faces:\n",
        "            # cv2.rectangle(image, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
        "            # cropped = image[y - int(h/10):y + h + int(h/10), x - int(w/10):x + w + int(w/10)]\n",
        "            # cropped = image[y - 15: y +h +15, x-15 :x+w+15]\n",
        "            # cv2.imwrite(\"/content/drive/MyDrive/FaceShape Dataset/training_set/\"+ + str(imgNum) + \".jpg\", cropped)\n",
        "            if tem_w <= w:\n",
        "                cropped = image[y: y +h, x :x+w]\n",
        "            tem_w=w\n",
        "        cv2.imwrite(\"/content/drive/MyDrive/FaceShape Dataset/training_set/crop/\"+face+\"/\" + str(imgNum) + \".jpg\", cropped)\n",
        "\n",
        "        imgNum += 1\n",
        "\n",
        "        print(imgNum)\n",
        "    print(face)\n",
        "        # cv2_imshow(cropped)\n",
        "        # cv2.waitKey(0)\n",
        "        # cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "NikpKRBvBqA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from glob import glob\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "faceCascade = cv2.CascadeClassifier('/content/drive/MyDrive/haarcascade_frontalface_default.xml')\n",
        "\n",
        "face_name = ['Heart','Oblong','Oval','Round','Square']\n",
        "\n",
        "for face in face_name:\n",
        "    image_datas = glob('/content/drive/MyDrive/FaceShape Dataset/testing_set/'+face+'/*.jpg')\n",
        "    imgNum  = 0\n",
        "    for imgpath in image_datas:\n",
        "        print(imgpath)\n",
        "        image = cv2.imread(imgpath)\n",
        "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "        faces = faceCascade.detectMultiScale(\n",
        "            gray,\n",
        "            scaleFactor=1.3,\n",
        "            minNeighbors=5,\n",
        "            minSize=(30, 30)\n",
        "        )\n",
        "        tem_w = 0\n",
        "        \n",
        "        for (x, y, w, h) in faces:\n",
        "            # cv2.rectangle(image, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
        "            # cropped = image[y - int(h/10):y + h + int(h/10), x - int(w/10):x + w + int(w/10)]\n",
        "            # cropped = image[y - 15: y +h +15, x-15 :x+w+15]\n",
        "            # cv2.imwrite(\"/content/drive/MyDrive/FaceShape Dataset/training_set/\"+ + str(imgNum) + \".jpg\", cropped)\n",
        "            if tem_w <= w:\n",
        "                cropped = image[y: y +h, x :x+w]\n",
        "            tem_w=w\n",
        "        cv2.imwrite(\"/content/drive/MyDrive/FaceShape Dataset/crop_set/testing_set/\"+face+\"/\" + str(imgNum) + \".jpg\", cropped)\n",
        "\n",
        "        imgNum += 1\n",
        "\n",
        "        print(imgNum)\n",
        "    print(face)\n",
        "        # cv2_imshow(cropped)\n",
        "        # cv2.waitKey(0)\n",
        "        # cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "cfrUuVhLoOyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ⬇크롭 이미지로 모델 돌리기"
      ],
      "metadata": {
        "id": "hlYx9KlepfJu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, val_df, test_df = split_train_val(\"/content/drive/MyDrive/FaceShape Dataset/crop_set/\")"
      ],
      "metadata": {
        "id": "1cP5VHmSpioH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_examples()"
      ],
      "metadata": {
        "id": "0-DC64nOpt9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configs\n",
        "class args:\n",
        "    data_dir=\"/content/drive/MyDrive/FaceShape Dataset/crop_set/\"\n",
        "    batch_size=32\n",
        "    n_epochs=20\n",
        "    learning_rate=0.001\n",
        "    debug=False"
      ],
      "metadata": {
        "id": "mIYfWQ1Vqdlj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, val_df, test_df = split_train_val(args.data_dir)\n",
        "if args.debug:\n",
        "    train_df, val_df = train_df.sample(n=10).reset_index(drop=True), val_df.sample(n=10).reset_index(drop=True)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    FaceShapeDataset(train_df, transform=transform, split=\"train\"),\n",
        "    batch_size=args.batch_size, shuffle=True\n",
        ")\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    FaceShapeDataset(val_df, transform=transform, split=\"val\"),\n",
        "    batch_size=args.batch_size, shuffle=False\n",
        ")\n",
        "\n",
        "import time\n",
        "from datetime import timedelta\n",
        "st = time.time()\n",
        "print(\"-------- Start training --------\")\n",
        "model = EffNet().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
        "model = train(model, train_loader, val_loader, criterion, optimizer, epochs=args.n_epochs, device=device)\n",
        "print(\"-------- End training, time taken:\", timedelta(seconds=int(time.time()-st)))"
      ],
      "metadata": {
        "id": "6fmWBxREqnir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_ds = FaceShapeDataset(test_df, transform=transform, split=\"test\")\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_ds,\n",
        "    batch_size=args.batch_size, shuffle=False\n",
        ")\n",
        "\n",
        "model.eval()\n",
        "test_loss = 0\n",
        "correct = 0\n",
        "y_true = []\n",
        "y_pred = []\n",
        "with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        output = model(data)\n",
        "        test_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
        "        pred = output.argmax(dim=1, keepdim=True)\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "        y_true += target.view(-1).tolist()\n",
        "        y_pred += pred.view(-1).tolist()\n",
        "test_loss /= len(test_loader.dataset)\n",
        "print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "    test_loss, correct, len(test_loader.dataset),\n",
        "    100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "y_true = [test_ds.idx2label[i] for i in y_true]\n",
        "y_pred = [test_ds.idx2label[i] for i in y_pred]\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_true, y_pred))"
      ],
      "metadata": {
        "id": "HfWXl0Usqt3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sn; sn.set(font_scale=1.4)\n",
        "\n",
        "CM = confusion_matrix(y_true, y_pred)\n",
        "class_names = list(test_ds.label2idx.keys())\n",
        "ax = plt.axes()\n",
        "sn.heatmap(CM, annot=True, \n",
        "           annot_kws={\"size\": 10}, \n",
        "           xticklabels=class_names, \n",
        "           yticklabels=class_names, ax = ax)\n",
        "ax.set_title('Confusion matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Qqj1gT6Dqw2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from random import randint\n",
        "\n",
        "list_idx = [randint(0, len(test_ds)) for i in range(25)]\n",
        "\n",
        "def display_examples():\n",
        "    \n",
        "    \"\"\"\n",
        "        Display 25 images from the images and labels\n",
        "    \"\"\"\n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "    fig.suptitle(\"Some examples of images of the test set\", fontsize=16)\n",
        "    for i,idx in enumerate(list_idx):\n",
        "        plt.subplot(5,5,i+1)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        plt.grid(False)\n",
        "        img_path = test_ds.df.iloc[idx][\"path\"]\n",
        "        image = cv2.imread(img_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        \n",
        "        plt.imshow(image, cmap=plt.cm.binary)\n",
        "        plt.xlabel(y_true[idx])\n",
        "    plt.show()\n",
        "\n",
        "display_examples()"
      ],
      "metadata": {
        "id": "W4_1B7Ijqz0I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}